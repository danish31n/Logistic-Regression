{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Q1 What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "\n",
        "\n",
        "**1. Logistic Regression:**\n",
        "- **Purpose**: Logistic regression is used for **binary classification**, where the goal is to predict a binary outcome (e.g., yes/no, 0/1, true/false).\n",
        "\n",
        "- **Output**: The output is a probability that the target variable belongs to a particular class, which is then mapped to a binary value (0 or 1). The probability is calculated using the **logistic function (sigmoid)**, which ensures that the output lies between 0 and 1.\n",
        "  - Formula:  $P(y = 1 | X) = \\frac{1}{1 + e^{-(b_0 + b_1X)}}$\n",
        "- **Nature of the Model**: It's based on the **logistic function**, which transforms the linear combination of input features into a probability.\n",
        "- **Use Case**: It’s primarily used in cases where the outcome is categorical, especially when the categories are mutually exclusive (e.g., spam vs. not spam).\n",
        "\n",
        "**2. Linear Regression:**\n",
        "- **Purpose**: Linear regression is used for **predicting continuous values**. It models the relationship between the dependent variable and one or more independent variables.\n",
        "- **Output**: The output is a **real number**. The model estimates the value of the target variable by a linear combination of input features.\n",
        "  - Formula: $Y = b_0 + b_1X_1 + b_2X_2 + ... + b_nX_n$\n",
        "- **Nature of the Model**: It’s based on fitting a **linear equation** to the data.\n",
        "\n",
        "- **Use Case**: It’s used in cases where the outcome is continuous (e.g., predicting house prices, temperatures, etc.).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Q2 What is the mathematical equation of Logistic Regression?\n",
        "\n",
        "The mathematical equation of **Logistic Regression** is:\n",
        "\n",
        "### **1. Linear Model:**\n",
        "First, compute the linear relationship:\n",
        "\n",
        "$ z = b_0 + b_1 x_1 + b_2 x_2 + \\cdots + b_n x_n $\n",
        "\n",
        "where:  \n",
        "- $ z $ = linear combination of input features  \n",
        "- $ b_0 $ = intercept (bias)  \n",
        "- $ b_i $ = coefficients (weights)  \n",
        "- $ x_i $ = input features  \n",
        "\n",
        "### **2. Sigmoid Function:**\n",
        "Apply the **sigmoid function** to convert the linear output into a probability:\n",
        "\n",
        "$ P(y = 1 \\mid x) = \\frac{1}{1 + e^{-z}} $\n",
        "\n",
        "Thus, the complete logistic regression equation is:\n",
        "\n",
        "$ P(y = 1 \\mid x) = \\frac{1}{1 + e^{-(b_0 + b_1 x_1 + b_2 x_2 + \\cdots + b_n x_n)}} $\n",
        "\n",
        "where the output is a probability between **0** and **1**.\n",
        "\n",
        "### **Interpretation:**  \n",
        "- If $ P(y = 1 \\mid x) \\geq 0.5 $ → Predict class **1**  \n",
        "- If $ P(y = 1 \\mid x) < 0.5 $ → Predict class **0**  \n",
        "\n",
        "\n",
        "\n",
        "#Q3 Why do we use the Sigmoid function in Logistic Regression?\n",
        "\n",
        "\n",
        "We use the **sigmoid function** in **Logistic Regression** because it maps any real-valued input to a value between **0 and 1**, which makes it suitable for modeling **probabilities**.\n",
        "\n",
        "### **Reasons for using the Sigmoid function:**\n",
        "1. **Probability Interpretation:**  \n",
        "   Logistic regression predicts the probability that a given input belongs to a particular class (e.g., 0 or 1). The sigmoid function ensures that the output value is between 0 and 1, which aligns perfectly with the definition of probability:\n",
        "\n",
        "   $ \\sigma(z) = \\frac{1}{1 + e^{-z}} $\n",
        "\n",
        "2. **Non-linearity:**  \n",
        "   The sigmoid function introduces **non-linearity** into the model, which allows logistic regression to handle complex relationships between input variables and output probabilities.\n",
        "\n",
        "3. **Smooth Gradient for Optimization:**  \n",
        "   - The sigmoid function is **differentiable** and has a smooth gradient, which makes it easier to optimize using methods like **gradient descent**.  \n",
        "   - The gradient of the sigmoid function is:\n",
        "\n",
        "   $ \\sigma'(z) = \\sigma(z)(1 - \\sigma(z)) $\n",
        "\n",
        "   This ensures that the gradient does not vanish completely, allowing the model to learn effectively.\n",
        "\n",
        "4. **Thresholding for Classification:**  \n",
        "   The sigmoid function outputs a probability between 0 and 1, which allows for easy thresholding at **0.5** to make a binary classification decision:\n",
        "\n",
        "   $ \\hat{y} =\n",
        "   \\begin{cases}\n",
        "   1, & \\text{if } \\sigma(z) \\geq 0.5 \\\\\n",
        "   0, & \\text{if } \\sigma(z) < 0.5\n",
        "   \\end{cases} $\n",
        "\n",
        "5. **Link to Log-Loss (Cross-Entropy Loss):**  \n",
        "   Logistic regression uses the sigmoid function because it pairs naturally with the **log-loss** (or **cross-entropy loss**) function, which measures the difference between predicted probabilities and actual labels.  \n",
        "\n",
        "\n",
        "#Q4 What is the cost function of Logistic Regression?\n",
        "\n",
        "\n",
        "The **cost function** (also known as the **loss function**) of **Logistic Regression** is used to quantify how well the model's predictions match the actual labels. In logistic regression, the cost function is typically based on **log-likelihood** and is called the **cross-entropy loss** or **logistic loss**. This cost function is designed to penalize the model more heavily for making incorrect predictions with high confidence.\n",
        "\n",
        "### The Logistic Regression Cost Function (Cross-Entropy Loss)\n",
        "\n",
        "Given the binary classification problem, the cost function for **Logistic Regression** is defined as:\n",
        "\n",
        "$\n",
        "J(\\mathbf{w}, b) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_{\\mathbf{w}, b}(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_{\\mathbf{w}, b}(x^{(i)})) \\right]\n",
        "$\n",
        "\n",
        "Where:\n",
        "- $ J(\\mathbf{w}, b) $ is the cost function (or loss function),\n",
        "- $ m $ is the number of training examples,\n",
        "\n",
        "- $ y^{(i)} $ is the actual label of the $ i $-th training example (0 or 1),\n",
        "- $ x^{(i)} $ is the input features of the $ i $-th training example,\n",
        "- $ h_{\\mathbf{w}, b}(x^{(i)}) $ is the predicted probability (model output) for the $ i $-th example, given by the **sigmoid function**:\n",
        "\n",
        "$\n",
        "h_{\\mathbf{w}, b}(x^{(i)}) = \\sigma(\\mathbf{w}^T x^{(i)} + b) = \\frac{1}{1 + e^{-(\\mathbf{w}^T x^{(i)} + b)}}\n",
        "$\n",
        "\n",
        "### Breaking Down the Cost Function:\n",
        "\n",
        "1. **For Correct Predictions**:\n",
        "   - When the true label $ y^{(i)} = 1 $, the cost function includes $ \\log(h_{\\mathbf{w}, b}(x^{(i)})) $, which penalizes the model based on how close $ h_{\\mathbf{w}, b}(x^{(i)}) $ is to 1.\n",
        "\n",
        "   - When the true label $ y^{(i)} = 0 $, the cost function includes $ \\log(1 - h_{\\mathbf{w}, b}(x^{(i)})) $, which penalizes the model based on how close $ h_{\\mathbf{w}, b}(x^{(i)}) $ is to 0.\n",
        "\n",
        "2. **For Incorrect Predictions**:\n",
        "   - If the model is highly confident in the wrong prediction (e.g., $ h_{\\mathbf{w}, b}(x^{(i)}) $ is close to 1 when $ y^{(i)} = 0 $, or close to 0 when $ y^{(i)} = 1 $), the logarithmic terms $ \\log(h_{\\mathbf{w}, b}(x^{(i)})) $ or $ \\log(1 - h_{\\mathbf{w}, b}(x^{(i)})) $ will result in a large penalty.\n",
        "\n",
        "3. **Averaging the Cost**:\n",
        "   - The cost for all training examples is averaged by dividing by $ m $, the number of training examples, to prevent the cost from becoming too large as the number of examples increases.\n",
        "\n",
        "### Why This Cost Function?\n",
        "\n",
        "- **Probabilistic Interpretation**: The cost function is based on the **log-likelihood** of the logistic regression model, treating the output as a probability. The cost penalizes the model more heavily for confident but incorrect predictions.\n",
        "\n",
        "- **Convexity**: The cost function is **convex** with respect to the model parameters, meaning there is only one global minimum, making it suitable for optimization via methods like **Gradient Descent**.\n",
        "- **Differentiability**: The cross-entropy cost function is differentiable, which allows efficient optimization during training (using techniques like gradient descent).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Q5 What is Regularization in Logistic Regression? Why is it needed?\n",
        "\n",
        "\n",
        "\n",
        "**Regularization** in **Logistic Regression** is a technique used to prevent **overfitting**, which occurs when the model becomes too complex and fits the noise or random fluctuations in the training data rather than the underlying trend. Regularization helps improve the generalization ability of the model, making it more likely to perform well on unseen data (test set).\n",
        "\n",
        "### Why is Regularization Needed?\n",
        "\n",
        "In the context of **Logistic Regression**, regularization is important for several reasons:\n",
        "\n",
        "1. **Prevent Overfitting**:\n",
        "   - When a model has too many features or overly large weights, it may fit the training data extremely well (low training error) but fail to generalize to new, unseen data (high test error). Regularization penalizes large weights, which helps in reducing the complexity of the model.\n",
        "   \n",
        "2. **Simplify the Model**:\n",
        "   - Regularization encourages simpler models with smaller coefficient values. This often leads to better performance on unseen data since simpler models tend to generalize better than complex ones.\n",
        "\n",
        "3. **Improving Interpretability**:\n",
        "   - Regularization can help in identifying the most important features by shrinking the less important ones toward zero, making the model easier to interpret.\n",
        "\n",
        "4. **Handling Multicollinearity**:\n",
        "   - In situations where there is multicollinearity (high correlation between input features), regularization can help by shrinking coefficients of correlated features, reducing their impact and improving model stability.\n",
        "\n",
        "### Types of Regularization in Logistic Regression\n",
        "\n",
        "There are two main types of regularization commonly used in **Logistic Regression**: **L1 Regularization** and **L2 Regularization**.\n",
        "\n",
        "#### 1. **L2 Regularization (Ridge Regularization)**\n",
        "\n",
        "L2 regularization adds a penalty equal to the **sum of the squares of the coefficients** to the cost function. The objective function with L2 regularization becomes:\n",
        "\n",
        "$\n",
        "J(\\mathbf{w}, b) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_{\\mathbf{w}, b}(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_{\\mathbf{w}, b}(x^{(i)})) \\right] + \\lambda \\sum_{j=1}^{n} w_j^2\n",
        "$\n",
        "\n",
        "Where:\n",
        "- $ \\lambda $ is the **regularization parameter** (also called **hyperparameter**), which controls the strength of the regularization (larger $ \\lambda $ means more regularization).\n",
        "- $ w_j $ represents the weights associated with the input features.\n",
        "  \n",
        "**Effect**:\n",
        "- L2 regularization penalizes large weights, but it does not force any of them to become exactly zero. Instead, it encourages all weights to be smaller, leading to a more balanced and less complex model.\n",
        "\n",
        "#### 2. **L1 Regularization (Lasso Regularization)**\n",
        "\n",
        "L1 regularization adds a penalty equal to the **sum of the absolute values of the coefficients** to the cost function. The objective function with L1 regularization becomes:\n",
        "\n",
        "$\n",
        "J(\\mathbf{w}, b) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_{\\mathbf{w}, b}(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_{\\mathbf{w}, b}(x^{(i)})) \\right] + \\lambda \\sum_{j=1}^{n} |w_j|\n",
        "$\n",
        "\n",
        "Where:\n",
        "- $ \\lambda $ is the regularization parameter, controlling the amount of regularization.\n",
        "  \n",
        "**Effect**:\n",
        "- L1 regularization has the property of driving some coefficients exactly to zero, effectively **selecting a subset of features**. This can lead to sparse models (with fewer non-zero weights) and is useful for **feature selection**.\n",
        "\n",
        "### How Does Regularization Work?\n",
        "\n",
        "1. **Penalty Term**:\n",
        "   - In both L1 and L2 regularization, a penalty term is added to the cost function. This term discourages the coefficients from becoming too large, thus preventing overfitting.\n",
        "   \n",
        "2. **Strength of Regularization**:\n",
        "   - The strength of the regularization is controlled by the parameter $ \\lambda $. A small $ \\lambda $ value means little regularization (similar to no regularization), while a large $ \\lambda $ value heavily penalizes large weights.\n",
        "   \n",
        "   - The regularization term $ \\lambda $ ensures that we don't place too much importance on any single feature, which could lead to overfitting if the model becomes too complex.\n",
        "\n",
        "3. **Trade-Off**:\n",
        "   - There is a trade-off between **bias** and **variance** when choosing the regularization strength. A higher $ \\lambda $ reduces variance (overfitting) but increases bias (underfitting). Proper tuning of $ \\lambda $ through techniques like **cross-validation** helps achieve the right balance.\n",
        "\n",
        "\n",
        "\n",
        "### Why is Regularization Important in Logistic Regression?\n",
        "\n",
        "1. **Prevents Overfitting**: By penalizing large weights, regularization prevents the model from fitting noise in the data, leading to better generalization to unseen data.\n",
        "   \n",
        "2. **Improves Generalization**: A regularized model will typically perform better on new, unseen data because it is less complex and avoids overfitting the training data.\n",
        "\n",
        "3. **Feature Selection (L1)**: L1 regularization can automatically perform feature selection by shrinking some coefficients to zero, which can be useful in cases with many features, potentially improving model interpretability and reducing computation time.\n",
        "\n",
        "4. **Control Model Complexity**: Regularization helps in controlling the complexity of the logistic regression model, ensuring that it doesn't become too complicated or sensitive to minor variations in the training data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Q6 Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "\n",
        "\n",
        "**Lasso**, **Ridge**, and **Elastic Net** regression are all regularization techniques used in **linear regression** (and can also be applied to **logistic regression**) to prevent overfitting by penalizing large coefficients. The key difference between these techniques lies in the **type of regularization** used and how they affect the model's coefficients.\n",
        "\n",
        "Here's a breakdown of the differences:\n",
        "\n",
        "### 1. **Ridge Regression (L2 Regularization)**\n",
        "\n",
        "**Ridge regression** adds a penalty to the cost function equal to the **sum of the squares of the coefficients**. This is known as **L2 regularization**.\n",
        "\n",
        "#### Cost Function:\n",
        "\\$\n",
        "J(\\mathbf{w}) = \\sum_{i=1}^{m} \\left[ (y^{(i)} - \\hat{y}^{(i)})^2 \\right] + \\lambda \\sum_{j=1}^{n} w_j^2\n",
        "\\$\n",
        "\n",
        "Where:\n",
        "- $ \\hat{y}^{(i)} $ is the predicted value for the $i$-th observation,\n",
        "- $ w_j $ are the coefficients of the model,\n",
        "- $ \\lambda $ is the regularization parameter (controls the strength of regularization).\n",
        "\n",
        "#### Characteristics:\n",
        "- Ridge regression **penalizes large coefficients** but does not eliminate them entirely. It **shrinks** the coefficients toward zero but **keeps them non-zero**.\n",
        "\n",
        "- It is useful when you have many features, and you believe all of them contribute in some way to the model, but you want to prevent any single feature from dominating.\n",
        "  \n",
        "#### Use Case:\n",
        "- Ridge regression is preferred when there is **multicollinearity** (when features are highly correlated with each other) because it helps stabilize the solution by reducing the impact of highly correlated features.\n",
        "\n",
        "\n",
        "\n",
        "### 2. **Lasso Regression (L1 Regularization)**\n",
        "\n",
        "**Lasso regression** adds a penalty to the cost function equal to the **sum of the absolute values of the coefficients**. This is known as **L1 regularization**.\n",
        "\n",
        "#### Cost Function:\n",
        "\n",
        "\\$\n",
        "J(\\mathbf{w}) = \\sum_{i=1}^{m} \\left[ (y^{(i)} - \\hat{y}^{(i)})^2 \\right] + \\lambda \\sum_{j=1}^{n} |w_j|\n",
        "\\$\n",
        "\n",
        "\n",
        "Where:\n",
        "- $ \\hat{y}^{(i)} $ is the predicted value for the $i$-th observation,\n",
        "- $ w_j $ are the coefficients of the model,\n",
        "- $ \\lambda $ is the regularization parameter.\n",
        "\n",
        "#### Characteristics:\n",
        "- Lasso regression **penalizes large coefficients** and **forces some coefficients to become exactly zero**. This means that Lasso is capable of performing **feature selection**, automatically eliminating unimportant features.\n",
        "\n",
        "- It is useful when you have **a large number of features** and suspect that only a subset of them are truly relevant. Lasso helps in identifying which features are important by setting others to zero.\n",
        "  \n",
        "#### Use Case:\n",
        "- Lasso regression is preferred when you want to **automatically select a subset of features** from a larger set, especially when there are many irrelevant or redundant features.\n",
        "\n",
        "\n",
        "\n",
        "### 3. **Elastic Net Regression**\n",
        "\n",
        "**Elastic Net** regression combines both **L1 (Lasso)** and **L2 (Ridge)** regularization. It is useful when there are **multiple correlated features**, as it can behave like **Ridge** when features are highly correlated and like **Lasso** when features are not.\n",
        "\n",
        "#### Cost Function:\n",
        "\n",
        "\\$\n",
        "J(\\mathbf{w}) = \\sum_{i=1}^{m} \\left[ (y^{(i)} - \\hat{y}^{(i)})^2 \\right] + \\lambda_1 \\sum_{j=1}^{n} |w_j| + \\lambda_2 \\sum_{j=1}^{n} w_j^2\n",
        "\\$\n",
        "\n",
        "\n",
        "Where:\n",
        "- $ \\lambda_1 $ is the regularization parameter for L1 (Lasso),\n",
        "- $ \\lambda_2 $ is the regularization parameter for L2 (Ridge),\n",
        "- $ w_j $ are the coefficients of the model.\n",
        "\n",
        "#### Characteristics:\n",
        "- Elastic Net allows you to tune both L1 and L2 penalties via two hyperparameters, $ \\lambda_1 $ and $ \\lambda_2 $.\n",
        "\n",
        "- Elastic Net is especially useful when there are **many correlated features** in the dataset. While Lasso might randomly select one feature from a group of correlated features, Elastic Net can select multiple correlated features and shrink them together.\n",
        "- Elastic Net is often preferred when you have **high-dimensional data** with a mix of **relevant** and **irrelevant** features and you believe that some correlations exist between the features.\n",
        "\n",
        "#### Use Case:\n",
        "- Elastic Net is a good choice when you have **a large number of correlated predictors** (features) and you want to combine the advantages of both **feature selection (Lasso)** and **shrinkage (Ridge)**.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Q7 When should we use Elastic Net instead of Lasso or Ridge?\n",
        "\n",
        "We should use **Elastic Net** instead of **Lasso** or **Ridge** when:\n",
        "\n",
        "1. **High Multicollinearity**  \n",
        "   - If the predictors (independent variables) are highly correlated with each other, Elastic Net performs better than Lasso because Lasso tends to select only one of the correlated variables and ignore the others, whereas Elastic Net can handle grouped or correlated features more effectively by allowing them to be selected together.\n",
        "\n",
        "2. **Combination of L1 and L2 Regularization Needed**  \n",
        "   - Elastic Net combines both **L1 regularization** (used in Lasso) and **L2 regularization** (used in Ridge).  \n",
        "     - L1 regularization (Lasso) promotes sparsity (selects only important features).  \n",
        "     - L2 regularization (Ridge) shrinks the coefficients of correlated features towards each other.  \n",
        "   - Elastic Net provides a balance between feature selection (L1) and coefficient shrinkage (L2), which can improve model performance when neither Ridge nor Lasso alone gives optimal results.\n",
        "\n",
        "3. **Lasso Instability in High-Dimensional Data**  \n",
        "   - In cases with **more features than observations** (high-dimensional data), Lasso tends to select only a small subset of features, leading to instability and increased variance.  \n",
        "   - Elastic Net helps by stabilizing the solution using the Ridge penalty component.\n",
        "\n",
        "4. **When Pure Lasso or Ridge Is Not Performing Well**  \n",
        "   - If Lasso is too aggressive in shrinking coefficients to zero or Ridge is too weak in performing feature selection, Elastic Net can provide a middle ground by tuning the balance between L1 and L2 using the mixing parameter **α**.\n",
        "\n",
        "\n",
        "\n",
        "#Q8 What is the impact of the regularization parameter $(λ)$ in Logistic Regression?\n",
        "\n",
        "\n",
        "In **Logistic Regression**, the regularization parameter $\\( \\lambda \\$) (or **C** in some implementations like `scikit-learn`) controls the strength of the penalty applied to the model’s coefficients, which impacts the model's complexity and performance.\n",
        "\n",
        "\n",
        "###  **Impact of the Regularization Parameter $\\( \\lambda \\$):**\n",
        "1. **High λ (Strong Regularization)**  \n",
        "   - Increases the strength of the penalty, forcing the coefficients to shrink toward zero.  \n",
        "   - Reduces model complexity → Prevents overfitting.  \n",
        "   - If λ is too large, it may lead to **underfitting** (high bias, low variance).  \n",
        "\n",
        "2. **Low λ (Weak Regularization)**  \n",
        "   - Reduces the effect of the penalty, allowing the model to fit the data more closely.  \n",
        "   - Increases model complexity → Prevents underfitting.  \n",
        "   - If λ is too small, it may lead to **overfitting** (low bias, high variance).  \n",
        "\n",
        "3. **λ = 0 (No Regularization)**  \n",
        "   - The model reduces to standard Logistic Regression without any penalty.  \n",
        "   - Prone to overfitting, especially in high-dimensional data.  \n",
        "\n",
        "\n",
        "\n",
        "###  **Effect of Regularization Type:**\n",
        "- **L1 Regularization (Lasso):** Encourages sparsity by driving some coefficients to **exactly zero** → Useful for feature selection.\n",
        "\n",
        "- **L2 Regularization (Ridge):** Shrinks coefficients toward zero but **does not make them exactly zero** → Helps with multicollinearity.  \n",
        "- **Elastic Net:** Combines both L1 and L2 to balance between sparsity and shrinkage.  \n",
        "\n",
        "\n",
        "#Q9 What are the key assumptions of Logistic Regression?\n",
        "\n",
        "\n",
        "Logistic regression is a widely used statistical model for binary classification. Here are the key assumptions of logistic regression:\n",
        "\n",
        "1. **Binary Outcome**: Logistic regression assumes that the dependent variable (target) is binary, meaning it has two possible outcomes (e.g., 0 or 1, yes or no, true or false).\n",
        "\n",
        "2. **Linear Relationship between Predictors and Log-Odds**: It assumes that there is a linear relationship between the independent variables (predictors) and the log-odds of the dependent variable. This means that the log-odds of the event occurring is a linear combination of the predictors.\n",
        "\n",
        "3. **Independence of Observations**: Logistic regression assumes that the observations are independent of one another, meaning the outcome for one observation does not influence the outcome for another.\n",
        "\n",
        "4. **No or Little Multicollinearity**: Logistic regression assumes that the predictor variables are not highly correlated with each other. High multicollinearity (i.e., high correlation between predictors) can lead to unreliable estimates of the model coefficients.\n",
        "\n",
        "5. **Sufficient Sample Size**: Logistic regression assumes a large enough sample size to provide reliable estimates of the model parameters. A small sample size can lead to overfitting or incorrect inferences.\n",
        "\n",
        "6. **Homoscedasticity (for some models)**: While homoscedasticity (constant variance of residuals) is not a strict assumption in logistic regression, it is still preferable for the model to behave similarly across different levels of the predictors.\n",
        "\n",
        "7. **No Strong Outliers**: Logistic regression assumes that there are no extreme outliers that disproportionately influence the model’s predictions. Outliers can distort the results of the regression.\n",
        "\n",
        "8. **Additivity of Effects**: Logistic regression assumes that the effect of each predictor on the outcome is additive, meaning the impact of each predictor variable is independent of the values of other predictors.\n",
        "\n",
        "These assumptions help ensure that logistic regression produces valid and reliable estimates. However, deviations from these assumptions may impact the accuracy or reliability of the model, requiring potential adjustments or alternative methods.\n",
        "\n",
        "\n",
        "#Q10 What are some alternatives to Logistic Regression for classification tasks?\n",
        "\n",
        "\n",
        "There are several alternative algorithms to **Logistic Regression** for classification tasks. Here are some common ones:\n",
        "\n",
        "### 1. **Decision Trees**\n",
        "   - **Overview**: Decision trees are a non-linear model that splits the data into subsets based on feature values. Each split is made to maximize information gain or minimize impurity (e.g., Gini index or entropy).\n",
        "   - **Advantages**: Easy to interpret and visualize, can handle both numerical and categorical data, no need for feature scaling.\n",
        "   - **Disadvantages**: Prone to overfitting, especially with deep trees. Can be unstable if small changes in data lead to large changes in the tree structure.\n",
        "\n",
        "### 2. **Random Forest**\n",
        "   - **Overview**: A Random Forest is an ensemble method that builds multiple decision trees and combines their predictions (via majority voting for classification). It mitigates the overfitting issue of decision trees by averaging multiple models.\n",
        "   - **Advantages**: Robust to overfitting, handles large datasets well, performs well with both numerical and categorical data.\n",
        "   - **Disadvantages**: Less interpretable compared to a single decision tree, can be computationally expensive.\n",
        "\n",
        "### 3. **Support Vector Machines (SVM)**\n",
        "   - **Overview**: SVM finds the optimal hyperplane that separates the data into different classes. It can handle both linear and non-linear classification problems using the **kernel trick**.\n",
        "   - **Advantages**: Works well for high-dimensional data and is effective when there is a clear margin of separation between classes.\n",
        "   - **Disadvantages**: Can be computationally expensive, especially with large datasets. Choosing the right kernel and regularization parameter can be challenging.\n",
        "\n",
        "### 4. **K-Nearest Neighbors (KNN)**\n",
        "   - **Overview**: KNN is a simple, instance-based learning algorithm that classifies new data points based on the majority class of its nearest neighbors in the feature space.\n",
        "   - **Advantages**: Simple to understand and implement, no training phase, works well with smaller datasets.\n",
        "   - **Disadvantages**: Computationally expensive during prediction (as it needs to compute distances to all points), sensitive to irrelevant features and the curse of dimensionality.\n",
        "\n",
        "### 5. **Naive Bayes**\n",
        "   - **Overview**: Naive Bayes is a probabilistic classifier based on Bayes' Theorem. It assumes that the features are conditionally independent, given the class label.\n",
        "   - **Advantages**: Simple, fast, and works well with high-dimensional data (e.g., text classification), often performs well even with smaller datasets.\n",
        "   - **Disadvantages**: The independence assumption is often unrealistic, which can degrade performance on some types of data.\n",
        "\n",
        "\n",
        "### 6. **Neural Networks (Deep Learning)**\n",
        "   - **Overview**: Neural networks consist of layers of interconnected nodes (neurons) that transform the input data through non-linear activations. Deep learning models are particularly useful for complex data like images, text, or audio.\n",
        "   - **Advantages**: Highly flexible and capable of modeling complex, non-linear relationships. Can handle large and high-dimensional datasets.\n",
        "   - **Disadvantages**: Requires large datasets to train effectively, computationally expensive, and difficult to interpret.\n",
        "\n",
        "\n",
        "### 7. **Logistic Regression with Regularization (Ridge, Lasso)**\n",
        "   - **Overview**: Regularized versions of logistic regression (Ridge for L2 regularization, Lasso for L1 regularization) help prevent overfitting by penalizing large coefficients.\n",
        "   - **Advantages**: Less prone to overfitting, good for high-dimensional data.\n",
        "   - **Disadvantages**: Still assumes a linear relationship between features and log-odds, so it might not work well if the data is highly non-linear.\n",
        "\n",
        "\n",
        "\n",
        "Each of these algorithms has its strengths and weaknesses, and the choice of which one to use depends on the nature of your data, the problem you're solving, and your computational resources.\n",
        "\n",
        "\n",
        "\n",
        "#Q11 What are Classification Evaluation Metrics?\n",
        "\n",
        "\n",
        "\n",
        "**Classification evaluation metrics** are tools used to assess the performance of a classification model. These metrics help us understand how well the model is predicting the target classes. Depending on the nature of the problem (binary or multi-class classification), the metrics can differ. Here are the key classification evaluation metrics:\n",
        "\n",
        "### 1. **Accuracy**\n",
        "   - **Definition**: The ratio of correctly predicted instances to the total instances.\n",
        "   - **Formula**:\n",
        "     \\$\n",
        "     Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "     \\$\n",
        "\n",
        "     where:\n",
        "     - **TP** = True Positives\n",
        "     - **TN** = True Negatives\n",
        "     - **FP** = False Positives\n",
        "     - **FN** = False Negatives\n",
        "\n",
        "   - **Usefulness**: Good for balanced datasets, but not ideal for imbalanced datasets.\n",
        "\n",
        "### 2. **Precision**\n",
        "   - **Definition**: The ratio of true positive predictions to the total predicted positives (i.e., how many of the predicted positive instances were actually positive).\n",
        "   - **Formula**:\n",
        "     \\$\n",
        "     Precision = \\frac{TP}{TP + FP}\n",
        "     \\$\n",
        "   - **Usefulness**: High precision indicates that the classifier is good at avoiding false positives.\n",
        "\n",
        "### 3. **Recall (Sensitivity or True Positive Rate)**\n",
        "   - **Definition**: The ratio of true positive predictions to the total actual positives (i.e., how many of the actual positive instances were correctly identified).\n",
        "   - **Formula**:\n",
        "     \\$\n",
        "     Recall = \\frac{TP}{TP + FN}\n",
        "     \\$\n",
        "   - **Usefulness**: High recall indicates that the classifier is good at identifying most of the positive instances.\n",
        "\n",
        "### 4. **F1-Score**\n",
        "   - **Definition**: The harmonic mean of precision and recall, providing a balance between the two.\n",
        "   - **Formula**:\n",
        "     \\$\n",
        "     F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}\n",
        "     \\$\n",
        "   - **Usefulness**: Useful when you want to balance precision and recall, especially in imbalanced datasets.\n",
        "\n",
        "### 5. **Specificity (True Negative Rate)**\n",
        "   - **Definition**: The ratio of true negative predictions to the total actual negatives (i.e., how many of the actual negative instances were correctly identified).\n",
        "   - **Formula**:\n",
        "     \\$\n",
        "     Specificity = \\frac{TN}{TN + FP}\n",
        "     \\$\n",
        "   - **Usefulness**: Indicates how well the model avoids false positives.\n",
        "\n",
        "### 6. **ROC-AUC (Receiver Operating Characteristic - Area Under Curve)**\n",
        "   - **Definition**: The ROC curve is a plot of the true positive rate (recall) versus the false positive rate (1 - specificity). AUC is the area under this curve, providing an overall assessment of model performance.\n",
        "   - **Usefulness**: AUC ranges from 0 to 1, with a value closer to 1 indicating a better model. It is particularly useful when dealing with imbalanced datasets.\n",
        "\n",
        "### 7. **Confusion Matrix**\n",
        "   - **Definition**: A matrix that shows the actual vs. predicted classifications, providing insight into the types of errors the model makes (false positives, false negatives, true positives, and true negatives).\n",
        "   - **Usefulness**: It helps visualize the performance and the types of errors made.\n",
        "\n",
        "### 8. **Log Loss (Logarithmic Loss)**\n",
        "   - **Definition**: A performance metric for evaluating classification models, especially in probabilistic classifications. It penalizes incorrect classifications with higher confidence.\n",
        "   - **Formula**:\n",
        "     \\$\n",
        "     Log \\, Loss = -\\frac{1}{N} \\sum_{i=1}^N [y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)]\n",
        "     \\$\n",
        "\n",
        "     where:\n",
        "     - $y_i$ is the true label (0 or 1),\n",
        "     - $p_i$ is the predicted probability of class 1.\n",
        "\n",
        "   - **Usefulness**: Ideal for probabilistic classifiers, and lower log loss is better.\n",
        "\n",
        "### 9. **Matthews Correlation Coefficient (MCC)**\n",
        "   - **Definition**: A measure of the quality of binary classifications. It accounts for true and false positives and negatives.\n",
        "   - **Formula**:\n",
        "     \\$\n",
        "     MCC = \\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}\n",
        "     \\$\n",
        "\n",
        "   - **Usefulness**: A balanced metric that works well even for imbalanced datasets. Ranges from -1 (worst) to 1 (best).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Q12 How does class imbalance affect Logistic Regression?\n",
        "\n",
        "\n",
        "Class imbalance can significantly affect Logistic Regression, and here's how:\n",
        "\n",
        "### 1. **Bias Toward Majority Class**\n",
        "   Logistic Regression models the probability of an outcome, and if one class is much more frequent than the other (i.e., class imbalance), the model might predict the majority class almost all the time. This results in high accuracy for the majority class but poor performance on the minority class. For example, if 95% of your data points belong to Class 0 and 5% to Class 1, the model may predict Class 0 most of the time, leading to a high accuracy but poor recall and precision for Class 1.\n",
        "\n",
        "### 2. **Skewed Decision Boundary**\n",
        "   The decision boundary learned by Logistic Regression is based on the likelihood ratio between classes. With imbalanced data, the model might shift the decision boundary toward the majority class, making it harder for the minority class to be correctly classified. In extreme cases, the model might fail to identify the minority class entirely.\n",
        "\n",
        "### 3. **Metrics Misleading**\n",
        "   With imbalanced classes, accuracy becomes less useful because it can be misleading. Even though the model might show high accuracy by simply predicting the majority class, it might be underperforming in terms of precision, recall, and F1-score for the minority class. In these situations, metrics like **Precision-Recall AUC** or **F1-score** are more informative.\n",
        "\n",
        "### 4. **Loss Function Impact**\n",
        "   Logistic Regression minimizes a **log-loss** function, which computes the error based on predicted probabilities. In imbalanced datasets, the loss is more influenced by the majority class, because the model is likely predicting it much more often. This leads to poor performance for the minority class as the model’s parameters are not updated enough to improve the classification of the minority class.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Q13 What is Hyperparameter Tuning in Logistic Regression?\n",
        "\n",
        "\n",
        "**Hyperparameter tuning** in logistic regression refers to the process of selecting the optimal hyperparameters (parameters set before training a model) to improve the model's performance. These hyperparameters control how the logistic regression model is trained and can significantly affect its accuracy, efficiency, and generalization to new data.\n",
        "\n",
        "In logistic regression, some common hyperparameters to tune include:\n",
        "\n",
        "1. **Regularization Strength (C)**:\n",
        "   - Logistic regression often uses regularization to prevent overfitting. The `C` parameter controls the regularization strength.\n",
        "   - **L2 regularization** is the most commonly used for logistic regression, and `C` is the inverse of the regularization strength.\n",
        "     - A smaller value of `C` implies stronger regularization (which reduces overfitting).\n",
        "     - A larger value of `C` implies weaker regularization (which might lead to overfitting).\n",
        "   - The goal is to find a balance between underfitting and overfitting.\n",
        "\n",
        "2. **Solver**:\n",
        "   - The solver determines the algorithm used for optimizing the model's coefficients. Common solvers include:\n",
        "     - **'liblinear'**: Uses a coordinate descent algorithm, suitable for small datasets and for problems with L1 regularization.\n",
        "     - **'lbfgs'**, **'newton-cg'**, and **'saga'**: These are other solvers often used with larger datasets or when L2 regularization is preferred.\n",
        "\n",
        "3. **Max Iterations** (`max_iter`):\n",
        "   - This hyperparameter specifies the maximum number of iterations for the optimization algorithm to converge.\n",
        "   - If the solver doesn't converge within the specified number of iterations, increasing this value might help.\n",
        "\n",
        "4. **Tolerance** (`tol`):\n",
        "   - This hyperparameter determines the tolerance level for stopping the optimization. A lower tolerance will result in more precise solutions but might require more iterations.\n",
        "\n",
        "5. **Penalty**:\n",
        "   - It specifies the type of regularization. For logistic regression, common choices are:\n",
        "     - **'l1'**: Lasso regularization that can lead to sparse models (feature selection).\n",
        "     - **'l2'**: Ridge regularization that adds a penalty on the magnitude of coefficients.\n",
        "     - **'elasticnet'**: A mix of L1 and L2 regularization.\n",
        "   - Regularization helps prevent overfitting by reducing the complexity of the model.\n",
        "\n",
        "### Why Tune Hyperparameters?\n",
        "\n",
        "Tuning hyperparameters allows the model to perform better by finding the configuration that leads to the best performance on validation data. Hyperparameter tuning is essential because:\n",
        "\n",
        "- It helps optimize model performance.\n",
        "- It can prevent overfitting or underfitting.\n",
        "- It ensures that the model generalizes well to new data.\n",
        "\n",
        "### Techniques for Hyperparameter Tuning:\n",
        "1. **Grid Search**:\n",
        "   - An exhaustive search over a predefined set of hyperparameters. It tries every combination of hyperparameters within the specified ranges.\n",
        "   \n",
        "2. **Random Search**:\n",
        "   - Randomly selects combinations of hyperparameters to try, which can be more efficient than grid search, especially with a large hyperparameter space.\n",
        "\n",
        "3. **Bayesian Optimization**:\n",
        "   - A more advanced technique that uses probability models to predict the most promising hyperparameters to try next.\n",
        "\n",
        "4. **Cross-Validation**:\n",
        "   - Cross-validation techniques (like k-fold cross-validation) are often used with hyperparameter tuning to get a more reliable estimate of model performance.\n",
        "\n",
        "### Example of Tuning Hyperparameters for Logistic Regression:\n",
        "\n",
        "             \n",
        "             from sklearn.model_selection import GridSearchCV\n",
        "             from sklearn.linear_model import LogisticRegression\n",
        "             \n",
        "             # Define model\n",
        "             model = LogisticRegression()\n",
        "             \n",
        "             # Define hyperparameters to tune\n",
        "             param_grid = {\n",
        "                 'C': [0.01, 0.1, 1, 10, 100],\n",
        "                 'solver': ['liblinear', 'lbfgs', 'saga'],\n",
        "                 'max_iter': [100, 200, 300],\n",
        "                 'penalty': ['l1', 'l2']\n",
        "             }\n",
        "             \n",
        "             # Perform grid search with cross-validation\n",
        "             grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
        "             grid_search.fit(X_train, y_train)\n",
        "             \n",
        "             # Best parameters\n",
        "             print(grid_search.best_params_)\n",
        "             \n",
        "\n",
        "In this example, the `GridSearchCV` is used to perform hyperparameter tuning by testing different combinations of values for `C`, `solver`, `max_iter`, and `penalty`.\n",
        "\n",
        "By adjusting these hyperparameters, logistic regression can become more effective in classifying data accurately.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Q14 What are different solvers in Logistic Regression? Which one should be used?\n",
        "\n",
        "\n",
        "In **Logistic Regression**, the choice of **solver** refers to the algorithm used to optimize the model’s parameters (coefficients) by minimizing the cost function (typically, the log-loss or binary cross-entropy for binary classification). Several solvers are available in the popular Python library `scikit-learn`, each having different strengths and weaknesses. Here are the common solvers:\n",
        "\n",
        "### 1. **‘liblinear’**\n",
        "   - **Algorithm**: Uses **coordinate descent** (a variant of gradient descent).\n",
        "   - **Pros**:\n",
        "     - Works well for small to medium datasets.\n",
        "     - Suitable for problems with a **small number of features** (e.g., less than 10,000 features).\n",
        "     - Handles **L1 regularization** (Lasso) well.\n",
        "   - **Cons**:\n",
        "     - May not scale well with large datasets.\n",
        "   - **Use Case**: When you have a small to medium-sized dataset and want to perform **L1 regularization** or need simplicity.\n",
        "\n",
        "### 2. **‘newton-cg’**\n",
        "   - **Algorithm**: Uses **Newton’s method**, which approximates the Hessian matrix for second-order optimization.\n",
        "   - **Pros**:\n",
        "     - More efficient for **large datasets** compared to `liblinear`.\n",
        "     - Works well with **L2 regularization** (Ridge).\n",
        "   - **Cons**:\n",
        "     - Can be slower for very small datasets.\n",
        "   - **Use Case**: When you have a medium to large dataset and want **L2 regularization**.\n",
        "\n",
        "### 3. **‘lbfgs’**\n",
        "   - **Algorithm**: A variation of **Broyden–Fletcher–Goldfarb–Shanno (BFGS)**, a quasi-Newton method.\n",
        "   - **Pros**:\n",
        "     - Generally performs well for **large datasets**.\n",
        "     - Suitable for both **L1 and L2 regularization**.\n",
        "   - **Cons**:\n",
        "     - Can be computationally expensive for very large datasets with many features.\n",
        "   - **Use Case**: Ideal for large datasets with **both regularization types**.\n",
        "\n",
        "### 4. **‘saga’**\n",
        "   - **Algorithm**: Similar to **‘lbfgs’**, but it’s a variant of **Stochastic Gradient Descent (SGD)** and more efficient for sparse data.\n",
        "   - **Pros**:\n",
        "     - Works well with **very large datasets**.\n",
        "     - Handles both **L1 and L2 regularization** (even **elastic net**).\n",
        "     - Efficient for **sparse data**.\n",
        "   - **Cons**:\n",
        "     - Can be slower than `liblinear` for smaller datasets.\n",
        "   - **Use Case**: Best for **very large datasets** or **sparse data**.\n",
        "\n",
        "### 5. **‘gd’ (Gradient Descent)**\n",
        "   - **Algorithm**: A first-order optimization technique, using iterative updates.\n",
        "   - **Pros**:\n",
        "     - Customizable learning rate, useful for experimenting.\n",
        "   - **Cons**:\n",
        "     - Might require careful tuning of learning rates and convergence criteria.\n",
        "   - **Use Case**: When you want more control over the optimization process, but it’s less common than the others.\n",
        "\n",
        "\n",
        "\n",
        "### **Which solver should you use?**\n",
        "\n",
        "- **For small datasets**: Use **‘liblinear’** if you need to use L1 regularization, or if you want a straightforward solver for a small-to-medium-sized dataset.\n",
        "- **For medium to large datasets**: Use **‘lbfgs’** or **‘newton-cg’** for better performance with L2 regularization, as they are faster and more scalable.\n",
        "- **For very large or sparse datasets**: Use **‘saga’**, especially if you are working with a large dataset and need to handle sparse data or want to use L1 regularization.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Q15 How is Logistic Regression extended for multiclass classification?\n",
        "\n",
        "\n",
        "Logistic Regression is inherently a binary classifier, meaning it predicts the probability of a sample belonging to one of two classes. However, it can be extended to handle multiclass classification problems, where there are more than two possible classes, through two primary techniques:\n",
        "\n",
        "### 1. **One-vs-Rest (OvR) or One-vs-All (OvA)**\n",
        "In this approach, for a classification problem with $K$ classes, $K$ separate binary classifiers are trained, each one corresponding to one of the classes. For each classifier:\n",
        "\n",
        "- It learns to distinguish one class from all the others (thus, the \"rest\" or \"all\" of the classes).\n",
        "- For class $k$, the classifier will predict if a sample belongs to class $k$ (positive class) or not (negative class).\n",
        "\n",
        "During prediction:\n",
        "- Each classifier computes the probability of the sample belonging to its class.\n",
        "- The class with the highest probability across all classifiers is selected as the final predicted class.\n",
        "\n",
        "### Example:\n",
        "If we have three classes: $C_1$, $C_2$, and $C_3$, we train three classifiers:\n",
        "- Classifier 1: Classifies $C_1$ vs. $C_2 \\cup C_3$\n",
        "- Classifier 2: Classifies $C_2$ vs. $C_1 \\cup C_3$\n",
        "- Classifier 3: Classifies $C_3$ vs. $C_1 \\cup C_2$\n",
        "\n",
        "The model then selects the class with the highest probability.\n",
        "\n",
        "### 2. **Softmax Regression (Multinomial Logistic Regression)**\n",
        "This method directly generalizes logistic regression to multiclass classification by using the **softmax function**. Instead of using a single logistic function (sigmoid), we use the softmax function to predict a probability distribution over $K$ classes.\n",
        "\n",
        "In multinomial logistic regression, the model calculates the probability of each class $k$ given the input features $\\mathbf{x}$:\n",
        "\n",
        "$$\n",
        "P(y = k | \\mathbf{x}) = \\frac{e^{\\mathbf{w}_k^T \\mathbf{x}}}{\\sum_{j=1}^{K} e^{\\mathbf{w}_j^T \\mathbf{x}}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $\\mathbf{w}_k$ is the weight vector for class $k$.\n",
        "- The denominator sums the exponential values for all classes to ensure the output probabilities sum to 1.\n",
        "\n",
        "### Steps in Softmax Regression:\n",
        "- **Training**: The model learns $K$ weight vectors, one for each class, by minimizing the cross-entropy loss.\n",
        "- **Prediction**: The class with the highest probability $P(y = k | \\mathbf{x})$ is chosen as the predicted class.\n",
        "\n",
        "### Comparison:\n",
        "- **One-vs-Rest**: More straightforward to implement, but it treats each class independently and might lead to less optimal classification boundaries since it doesn't account for relationships between the classes.\n",
        "- **Softmax Regression**: More elegant, as it models the relationship between all classes together. The model is trained jointly, and the decision boundaries are better suited for handling multiple classes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Q16 What are the advantages and disadvantages of Logistic Regression?\n",
        "\n",
        "\n",
        "\n",
        "1. **Simple and interpretable:**\n",
        "   - Logistic regression is easy to understand and interpret. The output is probabilistic, meaning you get a predicted probability of an outcome, making it suitable for binary classification tasks (e.g., spam vs. not spam).\n",
        "\n",
        "2. **Efficiency:**\n",
        "   - It’s computationally efficient, making it suitable for large datasets. The training time is generally faster compared to more complex models like decision trees or neural networks.\n",
        "\n",
        "3. **Probabilistic output:**\n",
        "   - The model provides probabilities, which can be useful when you need to assess the confidence level of predictions.\n",
        "\n",
        "4. **Works well with linearly separable data:**\n",
        "   - If the data is linearly separable, logistic regression tends to perform very well.\n",
        "\n",
        "5. **Less prone to overfitting:**\n",
        "   - Logistic regression with regularization (e.g., L2 regularization) helps prevent overfitting, especially in high-dimensional datasets.\n",
        "\n",
        "6. **Good baseline model:**\n",
        "   - It’s often used as a baseline for binary classification tasks. It’s simple enough to give a quick first impression of the data’s behavior.\n",
        "\n",
        "7. **Works well with high-dimensional data (with regularization):**\n",
        "   - Logistic regression can handle high-dimensional spaces, especially with regularization techniques (L1 or L2 regularization).\n",
        "\n",
        "\n",
        "\n",
        "**Disadvantages of Logistic Regression:**\n",
        "\n",
        "1. **Assumes linearity:**\n",
        "   - Logistic regression assumes a linear relationship between the independent variables and the log-odds of the dependent variable, which may not always be true in practice. It struggles when the decision boundary is nonlinear.\n",
        "\n",
        "2. **Sensitive to outliers:**\n",
        "   - Logistic regression can be sensitive to outliers in the data. Outliers can significantly affect the model's performance and accuracy.\n",
        "\n",
        "3. **Not suitable for complex relationships:**\n",
        "   - When the relationship between features and the outcome is complex, logistic regression might not perform well compared to more complex models (like decision trees or neural networks).\n",
        "\n",
        "4. **Requires feature engineering:**\n",
        "   - Logistic regression may require careful feature selection or transformation. If the features are not carefully selected or engineered, the model may not perform well.\n",
        "\n",
        "5. **Assumption of no multicollinearity:**\n",
        "   - Logistic regression assumes that the features are not highly correlated with each other (multicollinearity). If there is high correlation among features, it may cause problems in estimating the coefficients reliably.\n",
        "\n",
        "6. **Binary classification limitation:**\n",
        "   - Logistic regression is primarily used for binary classification problems. Although there are extensions (e.g., multinomial logistic regression) for multiclass classification, it is not as straightforward to implement as other algorithms like decision trees or random forests for multiclass tasks.\n",
        "\n",
        "7. **Not good with highly imbalanced datasets:**\n",
        "   - If one class is significantly more prevalent than the other, logistic regression may be biased towards the majority class unless techniques like oversampling, undersampling, or using class weights are applied.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Q17 What are some use cases of Logistic Regression?\n",
        "\n",
        "\n",
        "Logistic regression is a powerful statistical method widely used for binary classification tasks. Here are some common use cases:\n",
        "\n",
        "### 1. **Spam Detection**\n",
        "   - **Use case**: Identifying whether an email is spam or not.\n",
        "   - **Application**: By analyzing various features (like the content of the email, sender's information, and subject line), logistic regression can classify emails as spam (1) or not spam (0).\n",
        "\n",
        "### 2. **Medical Diagnosis**\n",
        "   - **Use case**: Predicting the presence or absence of a disease.\n",
        "   - **Application**: Logistic regression is used to predict whether a patient has a particular disease (e.g., cancer or diabetes) based on medical test results, age, gender, and other risk factors.\n",
        "\n",
        "### 3. **Customer Churn Prediction**\n",
        "   - **Use case**: Predicting whether a customer will leave a service or stay.\n",
        "   - **Application**: In telecom, banking, or subscription services, logistic regression can help identify whether a customer is likely to cancel their subscription based on historical usage data, customer support interactions, etc.\n",
        "\n",
        "### 4. **Credit Scoring**\n",
        "   - **Use case**: Determining whether an individual is likely to default on a loan.\n",
        "   - **Application**: Banks use logistic regression to predict the probability that a person will default on a loan based on features like income, credit history, loan amount, etc.\n",
        "\n",
        "### 5. **Marketing Campaign Effectiveness**\n",
        "   - **Use case**: Predicting whether a customer will respond to a marketing campaign.\n",
        "   - **Application**: Logistic regression can be used to classify customers as \"responders\" or \"non-responders\" to a particular marketing offer, helping businesses tailor their marketing strategies.\n",
        "\n",
        "### 6. **E-commerce Purchase Prediction**\n",
        "   - **Use case**: Predicting whether a customer will make a purchase.\n",
        "   - **Application**: E-commerce platforms use logistic regression to predict the likelihood of a customer making a purchase based on their browsing behavior, demographics, and past transactions.\n",
        "\n",
        "### 7. **Sentiment Analysis**\n",
        "   - **Use case**: Classifying text into categories like positive or negative sentiment.\n",
        "   - **Application**: Logistic regression can be used in text classification tasks, such as sentiment analysis of customer reviews (positive vs. negative).\n",
        "\n",
        "### 8. **Social Media Influence**\n",
        "   - **Use case**: Predicting whether a social media post will go viral.\n",
        "   - **Application**: Logistic regression can be used to predict whether a post will gain significant traction (likes, shares, comments) based on user behavior patterns and content features.\n",
        "\n",
        "### 9. **Risk Assessment**\n",
        "   - **Use case**: Identifying risk levels (high or low).\n",
        "   - **Application**: Logistic regression can be used in fields like insurance or finance to assess the risk of events happening (e.g., a high or low likelihood of a vehicle being involved in an accident).\n",
        "\n",
        "### 10. **Fraud Detection**\n",
        "   - **Use case**: Identifying fraudulent activities in transactions.\n",
        "   - **Application**: Logistic regression can be used to classify transactions as legitimate or fraudulent based on transaction data (e.g., transaction amount, time, location, and user profile).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Q18 What is the difference between Softmax Regression and Logistic Regression?\n",
        "\n",
        "\n",
        "Both **Softmax Regression** and **Logistic Regression** are used for classification problems, but they differ in the types of problems they are suited for and how they handle the output.\n",
        "\n",
        "Here’s a breakdown of the key differences:\n",
        "\n",
        "### 1. **Type of Classification:**\n",
        "   - **Logistic Regression** is typically used for **binary classification**, where the output is a probability of the sample belonging to one of two classes (0 or 1).\n",
        "   - **Softmax Regression** is an extension of Logistic Regression and is used for **multiclass classification**. It is used when there are more than two classes (e.g., classifying into one of several categories).\n",
        "\n",
        "### 2. **Output:**\n",
        "   - **Logistic Regression** outputs a single probability value between 0 and 1 for a binary classification problem. This probability indicates how likely the input belongs to the positive class (class 1).\n",
        "     \\$\n",
        "     P(y=1|X) = \\frac{1}{1 + e^{-(w^T X + b)}}\n",
        "     \\$\n",
        "   - **Softmax Regression** outputs a probability distribution over multiple classes. For a multiclass problem with \\(C\\) classes, it computes a probability for each class, and the sum of all probabilities will equal 1.\n",
        "     \\$\n",
        "     P(y=c|X) = \\frac{e^{w_c^T X + b_c}}{\\sum_{i=1}^{C} e^{w_i^T X + b_i}}\n",
        "     \\$\n",
        "     where $\\(C\\$) is the number of classes, and $\\(w_c, b_c\\$) represent the parameters for each class.\n",
        "\n",
        "### 3. **Number of Outputs:**\n",
        "   - **Logistic Regression** has a **single output** representing the probability for the positive class.\n",
        "   - **Softmax Regression** has **multiple outputs** (one for each class), each representing the probability of that particular class.\n",
        "\n",
        "### 4. **Loss Function:**\n",
        "   - **Logistic Regression** uses **binary cross-entropy loss** for binary classification.\n",
        "   - **Softmax Regression** uses **categorical cross-entropy loss** for multiclass classification.\n",
        "\n",
        "### 5. **Interpretation of Output:**\n",
        "   - **Logistic Regression** outputs a probability between 0 and 1, which is typically thresholded at 0.5 to classify as either class 0 or class 1.\n",
        "   - **Softmax Regression** gives a probability distribution across all classes, where the class with the highest probability is selected as the predicted class.\n",
        "\n",
        "### 6. **Generalization:**\n",
        "   - **Logistic Regression** is essentially a special case of Softmax Regression when the number of classes is 2.\n",
        "   - **Softmax Regression** generalizes Logistic Regression to handle cases with more than two classes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Q19 How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "\n",
        "\n",
        "\n",
        "Choosing between **One-vs-Rest (OvR)** and **Softmax** for multiclass classification depends on the characteristics of the problem and the type of model you're working with. Here are some key factors to consider when deciding which approach to use:\n",
        "\n",
        "### 1. **Model Type**\n",
        "   - **Softmax** is typically used with **neural networks** and models that directly output a probability distribution over classes. The Softmax function ensures that the predicted values are mutually exclusive and sum to 1, making it ideal when you want a single, exclusive class prediction.\n",
        "   - **One-vs-Rest (OvR)**, on the other hand, is commonly used with models like **logistic regression**, **SVMs**, and **random forests**. It involves training one binary classifier for each class, where the classifier predicts whether the sample belongs to that class or not, and you choose the class with the highest probability (or decision score).\n",
        "\n",
        "### 2. **Interpretability and Decision Boundaries**\n",
        "   - **Softmax**: This approach is better when you need the model to output a **probability distribution** and when the classes are **mutually exclusive** (i.e., each instance can only belong to one class). It tends to produce smoother decision boundaries between classes since the model directly optimizes for the joint probability of all classes.\n",
        "   - **OvR**: This method might result in **less smooth decision boundaries**, as each classifier is independently trained, meaning the decision boundary is defined separately for each class versus the rest. It can lead to **more complex decision regions** and may be less robust when classes are imbalanced or have overlapping distributions.\n",
        "\n",
        "### 3. **Class Imbalance**\n",
        "   - **Softmax**: Can be sensitive to **class imbalance**, especially if you are using a neural network. If some classes are significantly underrepresented, the model may not learn to distinguish them well.\n",
        "   - **OvR**: Can also suffer from class imbalance, but each individual binary classifier is only concerned with one class. However, if some classes are much more difficult to classify, each classifier might need extra tuning (like using class weights) to improve performance.\n",
        "\n",
        "### 4. **Computational Considerations**\n",
        "   - **Softmax**: In terms of training, Softmax is generally more **efficient** when dealing with a large number of classes, because it involves training a single classifier (e.g., a neural network) that can output probabilities for all classes simultaneously.\n",
        "   - **OvR**: Requires training a separate binary classifier for each class, leading to **increased computational overhead** if the number of classes is large. For instance, if there are 10 classes, you need to train 10 separate models (or at least 10 separate decision functions).\n",
        "\n",
        "### 5. **Model Scalability**\n",
        "   - **Softmax**: Works well when the number of classes is relatively small to moderate and you can train a model that outputs class probabilities efficiently.\n",
        "   - **OvR**: Can become less scalable as the number of classes increases because the number of binary classifiers grows linearly with the number of classes. For a large number of classes, this can be computationally expensive.\n",
        "\n",
        "### 6. **Performance**\n",
        "   - **Softmax**: Tends to perform better in terms of generalization for models like neural networks, as it directly models the relationships between the classes. It often provides more accurate predictions when the model has access to more data and can learn the complex dependencies between classes.\n",
        "   - **OvR**: Can work well for simpler models (like SVMs or logistic regression) and might be more suitable if the model assumes independent decision boundaries for each class.\n",
        "\n",
        "### Summary Table\n",
        "\n",
        "| **Factor**               | **One-vs-Rest (OvR)**                          | **Softmax**                                     |\n",
        "|--------------------------|------------------------------------------------|-------------------------------------------------|\n",
        "| **Model Type**            | Works well with SVMs, logistic regression, etc. | Works with neural networks (e.g., softmax layer) |\n",
        "| **Class Exclusivity**     | Independent classifiers for each class         | Mutually exclusive classes, one class at a time  |\n",
        "| **Interpretability**      | Each classifier gives a binary prediction      | Direct probability distribution across classes  |\n",
        "| **Decision Boundaries**   | Can have more complex and less smooth boundaries| Smooth, continuous decision boundaries          |\n",
        "| **Class Imbalance**       | Can handle imbalance with adjustments (e.g., class weights) | Sensitive to imbalance, needs careful tuning   |\n",
        "| **Computational Cost**    | Requires training many classifiers for large number of classes | More efficient, one classifier for all classes  |\n",
        "| **Scalability**           | Less scalable for many classes                 | Scales better with a large number of classes    |\n",
        "\n",
        "### When to Use Each:\n",
        "- **Use Softmax**:\n",
        "  - When you are working with neural networks.\n",
        "  - When you need a probability distribution over the classes.\n",
        "  - When classes are mutually exclusive.\n",
        "  - When you want smoother decision boundaries and joint optimization across all classes.\n",
        "\n",
        "- **Use One-vs-Rest**:\n",
        "  - When using simpler models like logistic regression, SVM, or random forests.\n",
        "  - When you prefer to break the problem down into simpler binary classification problems.\n",
        "  - When you have a large number of classes and need to scale the model efficiently.\n",
        "\n",
        "In practice, the choice often depends on the type of model you are using, the number of classes, and the complexity of your problem.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Q20 How do we interpret coefficients in Logistic Regression?\n",
        "\n",
        "\n",
        "\n",
        "In logistic regression, the coefficients represent the relationship between the predictor variables (features) and the log-odds of the outcome variable. Here's how we interpret them:\n",
        "\n",
        "### 1. **Log-Odds Interpretation**\n",
        "   - Logistic regression predicts the log-odds of an event happening. The log-odds are the logarithm of the odds (probability of event occurring / probability of event not occurring).\n",
        "   - The coefficients tell us how much the log-odds of the dependent variable change for a one-unit change in the predictor variable, while holding other variables constant.\n",
        "\n",
        "### 2. **Coefficient Interpretation (in terms of Odds)**\n",
        "   - For a given coefficient $ \\beta_i $ associated with a feature $ x_i $, the interpretation is as follows:\n",
        "     - If $ \\beta_i > 0 $: An increase in $ x_i $ will increase the odds of the event occurring.\n",
        "     - If $ \\beta_i < 0 $: An increase in $ x_i $ will decrease the odds of the event occurring.\n",
        "     - If $ \\beta_i = 0 $: The predictor $ x_i $ has no effect on the odds of the event.\n",
        "\n",
        "### 3. **Exponentiation of Coefficients (Exp($\\beta$))**\n",
        "   - To interpret the coefficients in terms of odds ratios, we take the **exponent** of the coefficient $ e^{\\beta_i} $. This gives the odds ratio:\n",
        "     - **Odds Ratio = Exp($\\beta$)**: The odds of the event happening for a one-unit increase in $ x_i $.\n",
        "     - If $ e^{\\beta_i} > 1 $: The odds of the event increase as $ x_i $ increases.\n",
        "     - If $ e^{\\beta_i} < 1 $: The odds of the event decrease as $ x_i $ increases.\n",
        "     - If $ e^{\\beta_i} = 1 $: There is no change in the odds of the event.\n",
        "\n",
        "### 4. **Example Interpretation**\n",
        "   Suppose we have the following logistic regression model:\n",
        "   $$\n",
        "   \\text{log-odds} = \\beta_0 + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2\n",
        "   $$\n",
        "   - Let's say $ \\beta_1 = 0.5 $ and $ \\beta_2 = -1.2 $.\n",
        "   - For a 1-unit increase in $ x_1 $, the odds of the event occurring are multiplied by $ e^{0.5} \\approx 1.65 $. So, a 1-unit increase in $ x_1 $ increases the odds of the event by 65%.\n",
        "   - For a 1-unit increase in $ x_2 $, the odds of the event occurring are multiplied by $ e^{-1.2} \\approx 0.30 $. So, a 1-unit increase in $ x_2 $ decreases the odds of the event by 70%.\n",
        "\n",
        "### 5. **Significance of Coefficients**\n",
        "   - In addition to the magnitude, the significance (p-value) of each coefficient is important. A low p-value (usually < 0.05) indicates that the coefficient is statistically significant, meaning that the feature is likely contributing to the prediction of the outcome variable.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YUzOsAsQ5PJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1 Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#loading dataset\n",
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "\n",
        "df = pd.DataFrame(data.data, columns= data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "#For binary classification we only need two features  (0 and 1)\n",
        "\n",
        "df = df[df['target'] != 2]\n",
        "\n",
        "x = df.iloc[:,:-1]\n",
        "y = df.iloc[:,-1]\n",
        "\n",
        "# Splitting into training and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3, random_state=1)\n",
        "\n",
        "# Scaling the features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "\n",
        "#Model training\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression()\n",
        "clf.fit(x_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(x_test)\n",
        "\n",
        "\n",
        "#Printing model Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy Score :\" , accuracy_score(y_test,y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wleucg4ZUvoy",
        "outputId": "8f135617-e743-4dd2-bb15-23f4bc23e679"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2 Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#loading dataset\n",
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "\n",
        "df = pd.DataFrame(data.data, columns= data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "df = df[df['target'] != 2]\n",
        "\n",
        "x = df.iloc[:,:-1]\n",
        "y = df.iloc[:,-1]\n",
        "\n",
        "\n",
        "# Splitting into training and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=1)\n",
        "\n",
        "\n",
        "# Model training with L1 regularization\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression(penalty='l1', solver = 'liblinear')\n",
        "clf.fit(x_train,y_train)\n",
        "\n",
        "y_pred = clf.predict(x_test)\n",
        "\n",
        "\n",
        "#Printing model Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy Score :\" , accuracy_score(y_test,y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UEXyWo8osWE",
        "outputId": "30b91497-02ac-44dd-d6b2-959c761dd522"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3 Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2').\n",
        "#   Print model accuracy and coefficients.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "x , y = make_classification(n_samples=1000 , n_features= 10 , n_redundant= 5, n_informative=5, n_classes= 2, random_state=1)\n",
        "\n",
        "\n",
        "\n",
        "# Splitting into training and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.25, random_state=1)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression(penalty= 'l2' , solver='lbfgs')\n",
        "clf.fit(x_train,y_train)\n",
        "\n",
        "y_pred = clf.predict(x_test)\n",
        "\n",
        "#Printing model Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy Score :\" , accuracy_score(y_test,y_pred))\n",
        "\n",
        "# Printing the coefficients of the model\n",
        "print(\"\\nCoefficients of the model (after L2 regularization):\\n\")\n",
        "print(clf.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXhkbrwVytKi",
        "outputId": "069d667f-902f-4ffb-a081-804804981c64"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score : 0.836\n",
            "\n",
            "Coefficients of the model (after L2 regularization):\n",
            "\n",
            "[[ 0.16744847 -0.62488117  0.45010096 -0.44172177  0.18914296 -0.11498123\n",
            "  -0.39445326  0.00839952 -0.50251247  0.22930837]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4 Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "x , y = make_classification(n_samples= 1000 , n_features= 10 , n_informative=5 ,n_redundant=5 , n_classes=2, random_state=1)\n",
        "\n",
        "\n",
        "# Splitting into training and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.25, random_state=1)\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression(penalty= 'elasticnet', solver='saga' , l1_ratio=0.5)\n",
        "clf.fit(x_train,y_train)\n",
        "\n",
        "y_pred = clf.predict(x_test)\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy Score :\" , accuracy_score(y_test,y_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VeTz6Sg2EnK",
        "outputId": "c36bc7db-1b65-4d5b-b703-f362f601a550"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score : 0.836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5 Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "x , y = make_classification(n_samples=1000 , n_features=10 , n_informative=5, n_redundant=5 , n_classes=2 , random_state=1)\n",
        "\n",
        "# Splitting into training and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3, random_state=1)\n",
        "\n",
        "#Model Training\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression(multi_class='ovr', max_iter=1000)\n",
        "clf.fit(x_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = clf.predict(x_test)\n",
        "\n",
        "# Accuracy Score\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy Score :\" , accuracy_score(y_test,y_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haQP6mV737Wl",
        "outputId": "bf4ecaef-6d2f-4921-f68e-dbae74027377"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score : 0.8433333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6 Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "x, y = make_classification(n_samples= 1000, n_features= 10, n_redundant=5, n_informative=5, n_classes= 2, random_state=1)\n",
        "\n",
        "# Splitting into training and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.25, random_state=1)\n",
        "\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "param  = {'penalty' : ['l1','l2', 'elasticnet'],'C': [1,2,10,20,30,40], 'l1_ratio': [0.2, 0.5, 0.8]}\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression(solver='saga', max_iter=500)\n",
        "\n",
        "clf = GridSearchCV(classifier, param_grid=param, cv=5, n_jobs=-1)\n",
        "\n",
        "clf.fit(x_train,y_train)\n",
        "\n",
        "# Print the best parameters found by GridSearchCV\n",
        "print(\"Best Parameters:\", clf.best_params_)\n",
        "\n",
        "# Evaluate accuracy with the best model\n",
        "y_pred = clf.predict(x_test)\n",
        "\n",
        "# Print accuracy score\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tAQbrXqA2HM",
        "outputId": "b8e4e263-56a3-4c86-fdc6-9eaa49338742"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'l1_ratio': 0.2, 'penalty': 'l1'}\n",
            "Accuracy Score: 0.836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7 Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "x, y = make_classification(n_samples=1000, n_features=10, n_redundant=5, n_informative=5, n_classes=2, random_state=1)\n",
        "\n",
        "# Feature scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x = scaler.fit_transform(x)\n",
        "\n",
        "# Initialize Stratified K-Fold Cross-Validation\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "# Set up Stratified K-Fold\n",
        "kfold = StratifiedKFold(n_splits=5)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(solver='lbfgs', max_iter=1000, random_state=1)\n",
        "\n",
        "\n",
        "# Perform Stratified K-Fold Cross-Validation\n",
        "cv_scores = cross_val_score(model, x,y, cv=kfold ,scoring='accuracy')\n",
        "\n",
        "\n",
        "\n",
        "# Print the accuracy for each fold\n",
        "print(\"Accuracy for each fold:\", cv_scores)\n",
        "\n",
        "# Print the average accuracy\n",
        "print(\"Average Accuracy:\", np.mean(cv_scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Xy6NT79G7cs",
        "outputId": "393a65fc-a3b5-4917-92ac-82c6195148f2"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for each fold: [0.82  0.815 0.81  0.79  0.78 ]\n",
            "Average Accuracy: 0.8029999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8 Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "df =  pd.read_csv('spotify.csv')\n",
        "\n",
        "df['popularity_class'] = np.where(df['Popularity'] > 51, 1,0)\n",
        "\n",
        "x = df[['Duration (ms)']]\n",
        "y = df['popularity_class']\n",
        "\n",
        "\n",
        "# Splitting into training and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3, random_state=1)\n",
        "\n",
        "#Scalling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "#Model training\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "model.fit(x_train,y_train)\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "#Printing model Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy Score :\" , accuracy_score(y_test,y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prOudT9TN_fF",
        "outputId": "455df205-0f46-4438-c8b8-e58c4ce21cc9"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score : 0.9545454545454546\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9 Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "x, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=10, n_classes=2, random_state=1)\n",
        "\n",
        "\n",
        "# Split into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=1)\n",
        "\n",
        "\n",
        "params = {'penalty': ('l1','l2','elasticnet'), 'C':[1,2,3,4,10,20,30], 'l1_ratio': [0.2,0.4,0.8]}\n",
        "\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "# randomcv = RandomizedSearchCV(n_iter= 5)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression(solver='saga', max_iter=1000)\n",
        "\n",
        "clf = RandomizedSearchCV(classifier, param_distributions=params,  n_iter=10,cv=5,n_jobs=-1)\n",
        "\n",
        "# Fit RandomizedSearchCV\n",
        "clf.fit(x_train,y_train)\n",
        "\n",
        "y_pred = clf.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "print('Accuracy :', accuracy_score(y_test,y_pred))\n",
        "print('The best parameter:', clf.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zAHLF6jas3v",
        "outputId": "7d2657f7-5c02-4261-b8e9-02db707ec5ca"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.916\n",
            "The best parameter: {'penalty': 'l2', 'l1_ratio': 0.8, 'C': 30}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10 Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "x, y = make_classification(n_samples=500, n_features=14, n_informative=7, n_redundant=7, n_classes=3, random_state=1)\n",
        "\n",
        "\n",
        "# Split into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "\n",
        "clf = OneVsOneClassifier(LogisticRegression(solver='lbfgs'))\n",
        "clf.fit(x_train,y_train)\n",
        "\n",
        "y_pred = clf.predict(x_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "#Printing model Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy Score :\" , accuracy_score(y_test,y_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PH-JnNtggJF",
        "outputId": "528c636d-514b-4825-87d3-628242e7dffd"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score : 0.8466666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#11 Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "x, y = make_classification(n_samples=500, n_features=10, n_informative=5, n_redundant=5, n_classes=2, random_state=1)\n",
        "\n",
        "# Split into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)\n",
        "\n",
        "#Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "#model training\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "model.fit(x_train,y_train)\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test,y_pred)\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Pred: 0\", \"Pred: 1\"], yticklabels=[\"True: 0\", \"True: 1\"])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "O1vyiIkclWhD",
        "outputId": "1ce2c694-5ec4-4694-f627-d5fcf9e03a20"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#12 Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "x, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=2, random_state=1)\n",
        "\n",
        "# Split into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=1)\n",
        "\n",
        "#Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "#model training\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "model.fit(x_train,y_train)\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "print(\"Precision Score:\", precision_score(y_test,y_pred))\n",
        "print(\"Recall Score:\", recall_score(y_test,y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test,y_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0b11GngqvQ-",
        "outputId": "6ff712cd-6a16-4a58-a3ff-f5b094ee40ab"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision Score: 0.9043478260869565\n",
            "Recall Score: 0.7761194029850746\n",
            "F1 Score: 0.8353413654618473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#13 Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "x , y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=2,weights=[0.9,0.1] ,random_state=1)\n",
        "\n",
        "\n",
        "# Split into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=1)\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(class_weight='balanced')\n",
        "model.fit(x_train,y_train)\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score,  confusion_matrix, classification_report\n",
        "print(\"Accuracy Score :\" , accuracy_score(y_test,y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "# Classification Report\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\\n\", class_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoKj20ulM5oU",
        "outputId": "f75d7a78-ac63-4425-cf60-5d0de8b30a97"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score : 0.828\n",
            "Confusion Matrix:\n",
            " [[186  38]\n",
            " [  5  21]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.83      0.90       224\n",
            "           1       0.36      0.81      0.49        26\n",
            "\n",
            "    accuracy                           0.83       250\n",
            "   macro avg       0.66      0.82      0.70       250\n",
            "weighted avg       0.91      0.83      0.85       250\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#14 Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "df = pd.read_csv('titanic.csv')    # loading titanic dataset\n",
        "df.isna().sum()      #checking for missing value\n",
        "\n",
        "df['Age'] = df['Age'].fillna(df['Age'].median()) #imputating missing value in age column\n",
        "df = df.dropna(subset=['Cabin', 'Embarked'])   #droping null values from cabin\n",
        "df.isna().sum() # again checking for null value (none found)\n",
        "\n",
        "# Convert categorical column 'Sex' to numerical values\n",
        "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
        "\n",
        "x = df[['Pclass', 'Age', 'Sex']]\n",
        "y = df['Survived']\n",
        "\n",
        "# Split into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=1)\n",
        "\n",
        "# Feature scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "model.fit(x_train,y_train)\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score,  confusion_matrix, classification_report\n",
        "print(\"Accuracy Score :\" , accuracy_score(y_test,y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "# Classification Report\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\\n\", class_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzFVYMxHRXUP",
        "outputId": "4ad3371f-6d20-4eaf-ba7f-95e80f7e4e62"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score : 0.7049180327868853\n",
            "Confusion Matrix:\n",
            " [[12 13]\n",
            " [ 5 31]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.48      0.57        25\n",
            "           1       0.70      0.86      0.78        36\n",
            "\n",
            "    accuracy                           0.70        61\n",
            "   macro avg       0.71      0.67      0.67        61\n",
            "weighted avg       0.71      0.70      0.69        61\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#15 Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "x , y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=2,random_state=1)\n",
        "\n",
        "\n",
        "# Split into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=1)\n",
        "\n",
        "\n",
        "\n",
        "# --- Model without scaling ---\n",
        "# Train Logistic Regression without scaling\n",
        "model_without_scaling = LogisticRegression(max_iter=1000)\n",
        "model_without_scaling.fit(x_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy without scaling\n",
        "y_pred_without_scaling = model_without_scaling.predict(x_test)\n",
        "accuracy_without_scaling = accuracy_score(y_test, y_pred_without_scaling)\n",
        "print(\"Accuracy Score (without scaling):\", accuracy_without_scaling)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# --- Apply Standardization (scaling) ---\n",
        "scaler = StandardScaler()\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "x_test_scaled = scaler.transform(x_test)\n",
        "\n",
        "# Train Logistic Regression with scaled data\n",
        "model_with_scaling = LogisticRegression(max_iter=1000)\n",
        "model_with_scaling.fit(x_train_scaled, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy with scaling\n",
        "y_pred_with_scaling = model_with_scaling.predict(x_test_scaled)\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "print(\"Accuracy Score (with scaling):\", accuracy_with_scaling)\n",
        "\n",
        "\n",
        "\n",
        "# Comparison of results\n",
        "print(\"\\nComparison of results:\")\n",
        "print(f\"Accuracy without scaling: {accuracy_without_scaling:.4f}\")\n",
        "print(f\"Accuracy with scaling: {accuracy_with_scaling:.4f}\")\n",
        "\n",
        "\n",
        "improvement = (accuracy_with_scaling - accuracy_without_scaling) / accuracy_without_scaling * 100\n",
        "print(f\"Accuracy Improvement: {improvement:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7d0B_PpSCQJ",
        "outputId": "3c55b580-9796-43cb-941f-bd195b0694c8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score (without scaling): 0.8433333333333334\n",
            "Accuracy Score (with scaling): 0.8433333333333334\n",
            "\n",
            "Comparison of results:\n",
            "Accuracy without scaling: 0.8433\n",
            "Accuracy with scaling: 0.8433\n",
            "Accuracy Improvement: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#16 Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "x , y = make_classification(n_samples=1000, n_features=18, n_informative=9, n_redundant=9, n_classes=2,random_state=1)\n",
        "\n",
        "\n",
        "# Split into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=1)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "model.fit(x_train,y_train)\n",
        "\n",
        "y_pred_prob = model.predict_proba(x_test)[:,-1]\n",
        "\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "print('ROC-AUC score: ',roc_auc_score(y_test, y_pred_prob))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKQ91e3hegnV",
        "outputId": "fde08c83-7853-41ed-8c04-71a96f8f262f"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC score:  0.953558202382144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#17 Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "x, y = make_classification(n_samples=1000, n_features=12, n_informative=6, n_redundant=6, n_classes=2, random_state=1)\n",
        "\n",
        "# Split into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=1)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression(C=0.5)\n",
        "clf.fit(x_train,y_train)\n",
        "\n",
        "ypred = clf.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "print('Accuracy score: ',accuracy_score(y_test, ypred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXmX5NKWlYYh",
        "outputId": "22d102fb-934c-46b8-ca98-7f16a5c9cc4b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score:  0.865\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#18 Write a Python program to train Logistic Regression and identify important features based on model coefficients.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "x, y = make_classification(n_samples=1000, n_features=12, n_informative=6, n_redundant=6, n_classes=5, random_state=1)\n",
        "\n",
        "# Split into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "\n",
        "coefficients = model.coef_   # Get the coefficients for each class\n",
        "\n",
        "\n",
        "# Create a DataFrame to view the feature importance for each class\n",
        "feature_importance = pd.DataFrame(coefficients, columns=[f'Feature {i}' for i in range(x_train.shape[1])])\n",
        "\n",
        "# Calculate the absolute values of the coefficients for each class to determine importance\n",
        "importance_df = feature_importance.abs()\n",
        "\n",
        "# Plot the importance of each feature across all classes (by averaging the absolute values of the coefficients)\n",
        "feature_importance_mean = importance_df.mean(axis=0)\n",
        "\n",
        "# Sort features by their importance (mean of absolute coefficients)\n",
        "sorted_features = feature_importance_mean.sort_values(ascending=False)\n",
        "\n",
        "# Display the sorted feature importance\n",
        "print(\"Feature Importance Based on Coefficients:\")\n",
        "print(sorted_features)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WR3-6YhMnfqp",
        "outputId": "7848faaf-199e-4eb8-a56c-cec6ebec6d5a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importance Based on Coefficients:\n",
            "Feature 7     0.735865\n",
            "Feature 2     0.458036\n",
            "Feature 10    0.372276\n",
            "Feature 0     0.332117\n",
            "Feature 11    0.315071\n",
            "Feature 4     0.301231\n",
            "Feature 1     0.290811\n",
            "Feature 6     0.257448\n",
            "Feature 8     0.167143\n",
            "Feature 3     0.130488\n",
            "Feature 5     0.094780\n",
            "Feature 9     0.054276\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#19 Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score.\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "x, y = make_classification(n_samples=1000, n_features=18, n_informative=9, n_redundant=9, n_classes=2, random_state=1)\n",
        "\n",
        "# Split into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "yprid = model.predict(x_test)\n",
        "\n",
        "\n",
        "# Calculate Cohen’s Kappa Score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "print(f\"Cohen’s Kappa Score: {cohen_kappa_score(y_test,yprid)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoyVAxUzpx1s",
        "outputId": "23a2c229-ea7b-4d4a-a48a-254a69774433"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen’s Kappa Score: 0.7862391449565798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#20 Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "x, y = make_classification(n_samples=1000, n_features=14, n_informative=7, n_redundant=7, n_classes=2, random_state=1)\n",
        "\n",
        "# Split into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# Get predicted probabilities for the positive class\n",
        "y_pred_proba = model.predict_proba(x_test)[:, 1]\n",
        "\n",
        "# Compute Precision-Recall curve\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
        "\n",
        "# Plot the Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, marker='.', label='Logistic Regression')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "AWREwvgcrGEe",
        "outputId": "c483fc09-f0ae-482e-821f-dd8738213d5c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#21 Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "x, y = make_classification(n_samples=1000, n_features=14, n_informative=7, n_redundant=7, n_classes=2, random_state=1)\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=1)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "for solver in solvers:\n",
        "  from sklearn.linear_model import LogisticRegression\n",
        "  clf = LogisticRegression(solver=solver, max_iter=500)\n",
        "  clf.fit(x_train,y_train)\n",
        "\n",
        "   # Get predicted probabilities for the positive class (class 1)\n",
        "  y_pred = clf.predict(x_test)\n",
        "\n",
        "  # Calculate accuracy\n",
        "  from sklearn.metrics import accuracy_score\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  print(f\"Solver: {solver}, Accuracy: {accuracy}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lZ9s7WXtjPt",
        "outputId": "9fbe024a-b4af-4b7f-923d-aab6fa663c95"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solver: liblinear, Accuracy: 0.844\n",
            "Solver: saga, Accuracy: 0.844\n",
            "Solver: lbfgs, Accuracy: 0.844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#22 Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC).\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#loading dataset\n",
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "\n",
        "df = pd.DataFrame(data.data, columns= data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "#For binary classification we only need two features  (0 and 1)\n",
        "\n",
        "df = df[df['target'] != 2]\n",
        "\n",
        "x = df.iloc[:,:-1]\n",
        "y = df.iloc[:,-1]\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=1)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression()\n",
        "clf.fit(x_train,y_train)\n",
        "\n",
        "y_p = clf.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "MCC = matthews_corrcoef(y_test, y_p)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {MCC}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2QclHoA0p8I",
        "outputId": "284e713f-7ad0-4c35-e10e-6eba781e5a4a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#23 Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#loading dataset\n",
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "\n",
        "df = pd.DataFrame(data.data, columns= data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "#For binary classification we only need two features  (0 and 1)\n",
        "\n",
        "df = df[df['target'] != 2]\n",
        "\n",
        "x = df.iloc[:,:-1]\n",
        "y = df.iloc[:,-1]\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=1)\n",
        "\n",
        "# Train Logistic Regression model without scaling\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf_without_scaling = LogisticRegression()\n",
        "clf_without_scaling.fit(x_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy without scaling\n",
        "y_pred_without_scaling = clf_without_scaling.predict(x_test)\n",
        "accuracy_without_scaling = accuracy_score(y_test, y_pred_without_scaling)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(f\"Accuracy Score Without Scaling: {accuracy_without_scaling:.4f}\")\n",
        "\n",
        "# Apply Standardization (scaling)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "x_test_scaled = scaler.transform(x_test)\n",
        "\n",
        "# Train Logistic Regression model with scaling\n",
        "clf_with_scaling = LogisticRegression()\n",
        "clf_with_scaling.fit(x_train_scaled, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy with scaling\n",
        "y_pred_with_scaling = clf_with_scaling.predict(x_test_scaled)\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(f\"Accuracy Score With Scaling: {accuracy_with_scaling:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQHRKHj83XSz",
        "outputId": "669e119f-2ee6-4753-fdc8-21cfa016c90a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score Without Scaling: 1.0000\n",
            "Accuracy Score With Scaling: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#24 Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#loading dataset\n",
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "\n",
        "df = pd.DataFrame(data.data, columns= data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "#For binary classification we only need two features  (0 and 1)\n",
        "\n",
        "df = df[df['target'] != 2]\n",
        "\n",
        "x = df.iloc[:,:-1]\n",
        "y = df.iloc[:,-1]\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid for C (regularization strength)\n",
        "param = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression(solver='liblinear')# Using 'liblinear' solver for binary classification\n",
        "\n",
        "clf = GridSearchCV(classifier, param_grid=param, cv=5, scoring='accuracy')\n",
        "clf.fit(x_train,y_train)\n",
        "\n",
        "\n",
        "print(\"The best parameter (C) found: \", clf.best_params_)\n",
        "\n",
        "# Evaluate the model with the best 'C' on the test set\n",
        "best_model = clf.best_estimator_\n",
        "y_pred = best_model.predict(x_test)\n",
        "\n",
        "# Calculate accuracy on the test set\n",
        "from sklearn.metrics import accuracy_score\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test set accuracy with the best 'C': {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zewWlw_k9ycl",
        "outputId": "88306e4b-5a18-4437-b3f6-4c432d8c0fea"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best parameter (C) found:  {'C': 0.001}\n",
            "Test set accuracy with the best 'C': 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#25 Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "x , y = make_classification(n_samples=10000, n_features=10, n_informative=5, n_redundant=5, n_classes= 2, random_state=1)\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(x,y , test_size= 0.30, random_state=1)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "xtrain = scaler.fit_transform(xtrain)\n",
        "xtest = scaler.transform(xtest)\n",
        "\n",
        "# Model training\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "log = LogisticRegression()\n",
        "log.fit(xtrain,ytrain)\n",
        "\n",
        "import joblib\n",
        "\n",
        "joblib.dump(log, 'logistic_regression_model.joblib')\n",
        "print('Model save to logistic_regression_model.joblib')\n",
        "\n",
        "load_model = joblib.load('logistic_regression_model.joblib')\n",
        "print('Model is loaded')\n",
        "\n",
        "predictions = load_model.predict(xtest)\n",
        "print(\"Predictions on the test set:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LopB48yPgSg",
        "outputId": "06299b9f-3e7b-4383-9704-5c8450b8f029"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model save to logistic_regression_model.joblib\n",
            "Model is loaded\n",
            "Predictions on the test set: [0 0 1 ... 0 1 0]\n"
          ]
        }
      ]
    }
  ]
}